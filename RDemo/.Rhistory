z = sample(1:length(rho), Ndata, replace=TRUE, prob=rho)
# draw each data point according to the cluster-specific
# likelihood of its component
x = cbind(rnorm(rep(NA,Ndata), mu[z,1], rep(sd,Ndata)),
rnorm(rep(NA,Ndata), mu[z,2], rep(sd,Ndata)))
# return the data.
# also return the cluster centers and means in case
# that is useful for comparison
list("x" = x, "z" = z, "mu" = mu)
}
ex7_crp_gibbs <- function(data, sd, initz) {
# Run a Gibbs sampler for a CRP Gaussian mixture model
# on the data
#
# Args:
#  data: an Ndata x D matrix of data points
#  sd: we assume the Gaussian likelihood around any
#      cluster mean has covariance matrix diag(sd^2,...,sd^2);
#      so this is the standard deviation in any direction
#  initz: vector of strictly positive integers; initial
#      assignments of data points to clusters; takes
#      values 1,...,K
#
# Returns:
#  Nothing
#
# Note:
#  Has only been tested on D=2
# supposedly a collection of colors that
# are easily visually separated on the screen
# obtained from: https://graphicdesign.stackexchange.com/questions/3682/where-can-i-find-a-large-palette-set-of-contrasting-colors-for-coloring-many-d
sep_colors = c("#023FA5","#7D87B9","#BEC1D4","#D6BCC0","#BB7784","yellow","#4A6FE3","#8595E1","#B5BBE3","#E6AFB9","#E07B91","#D33F6A", "#11C638","#8DD593","#C6DEC7","#EAD3C6","#F0B98D","#EF9708","#0FCFC0","#9CDED6","#D5EAE7","#F3E1EB","#F6C4E1","#F79CD4")
palette(sample(sep_colors, replace=FALSE))
# setup the plot into two parts:
# the data and the probabilities for Gibbs sampling
par(mar = rep(2,4))
layout(matrix(c(1,2), 2, 1, byrow=TRUE),
heights=c(4,1)
)
# don't exceed this many Gibbs iterations
maxIters = 1000
# the algorithm will pause and plot after this
# iteration number; 0 ensures it will plot right off
minPauseIter = 0
# just setting alpha for inference.
# a small alpha encourages a small number of clusters
alpha = 0.01
# dimension of the data points
data_dim = ncol(data)
# cluster-specific covariance matrix
Sig = diag(sd^2,data_dim)
# prior covariance matrix
Sig0 = diag(3^2,data_dim)
# cluster-specific precision (Sig^{-1})
Prec = solve(Sig)
# prior precision (Sig^{-1})
Prec0 = solve(Sig0)
# prior mean on cluster parameters
mu0 = matrix(c(0,0), ncol=2, byrow=TRUE)
# number of data points
Ndata = nrow(data)
# initialize the sampler
z = initz  # initial cluster assignments
counts = as.vector(table(z))  # initial data counts at each cluster
Nclust = length(counts)	  # initial number of clusters
# run the Gibbs sampler
for(iter in 1:maxIters) {
# take a Gibbs step at each data point
for(n in 1:Ndata) {
# get rid of the nth data point
c = z[n]
counts[c] = counts[c] - 1
# if the nth data point was the only point in a cluster,
# get rid of that cluster
if(counts[c]==0) {
counts[c] = counts[Nclust]
loc_z = (z==Nclust)
z[loc_z] = c
counts = counts[-Nclust]
Nclust = Nclust - 1
}
z[n] = -1  # ensures z[n] doesn't get counted as a cluster
# unnormalized log probabilities for the clusters
log_weights = rep(NA,Nclust+1)
# find the unnormalized log probabilities
# for each existing cluster
for(c in 1:Nclust) {
c_Precision = Prec0 + counts[c] * Prec
c_Sig = solve(c_Precision)
# find all of the points in this cluster
loc_z = which(z==c)
# sum all the points in this cluster
if(length(loc_z) > 1) {
sum_data = colSums(data[z == c,])
} else {
sum_data = data[z==c,]
}
c_mean = c_Sig %*% (Prec %*% sum_data + Prec0 %*% t(mu0))
log_weights[c] = log(counts[c]) + dmvnorm(data[n,], mean = c_mean, sigma = c_Sig + Sig, log = TRUE)
}
# find the unnormalized log probability
# for the "new" cluster
log_weights[Nclust+1] = log(alpha) + dmvnorm(data[n,], mean = mu0, sigma = Sig0 + Sig, log = TRUE)
# transform unnormalized log probabilities
# into probabilities
max_weight = max(log_weights)
log_weights = log_weights - max_weight
loc_probs = exp(log_weights)
loc_probs = loc_probs / sum(loc_probs)
# sample which cluster this point should
# belong to
newz = sample(1:(Nclust+1), 1, replace=TRUE, prob=loc_probs)
# if necessary, instantiate a new cluster
if(newz == Nclust + 1) {
counts = c(counts,0)
Nclust = Nclust + 1
}
z[n] = newz
# update the cluster counts
counts[newz] = counts[newz] + 1
# if desired, plot the progress of the sampler
if(iter >= minPauseIter) {
# in the top plot, plot the points,
# colored by cluster assignment in this sampler step
plot(data, col=z, pch=19)
# highlight which point is currently being sampled
points(data[n,1], data[n,2], col="black", pch=4, lwd=4, cex=4)
points(data[n,1], data[n,2], col="black", pch=19, lwd=1, cex=2)
# plot the cluster probabilities for the Gibbs sampler.
# color the probabilities by cluster
barplot(as.matrix(loc_probs,nrow=length(loc_probs)),
beside=FALSE,
horiz=TRUE,
ylim=c(0,1),
width=0.7,
col=palette(),
main = paste("Gibbs iter = ", toString(iter),
", n = ", toString(n),
", #clust (without n) = ", toString(length(loc_probs)-1),
", ",
sep="")
)
# in the bottom plot, plot the Gibbs probabilities and sample
u = loc_probs[newz] * runif(1)
uplot = cumsum(loc_probs)[newz] - u
# plot the uniform random number used to draw a probability
points(uplot, 1, pch=25, col="red", bg="red")
# Generate a new draw for each press of "enter"
# Press 'x' when finished.
# Enter a number to progress that many full
# Gibbs iterations into the future.
line <- readline()
if(line == "x") {
dev.off()
return("done")
} else if(line == "") {
} else {
minPauseIter = iter + as.numeric(line)
}
}
}
}
}
# generate a data set with 100 data points
data <- ex7_gen_data(Ndata=100,sd=1)
# run a CRP Gibbs sampler
# initialized with all data points in the same cluster
ex7_crp_gibbs(data=data$x, sd=1, initz=rep(1,100))
# Description:
#   This script shows cluster assignments according to a Dirichlet distribution with large K.
#
# Details:
#   Press enter to keep making draws from the random distribution.
#   Press "x" and then enter when you're done.
#
# License: GNU v3
#
# Date: October 19, 2016
#
# Authors:
#    Originally called ex4_largeK_count
#    Rcode was written by Tamara Broderick.
#    Rcode was modified by Jiali Lin.
#    Depts. of Statistics, Virginia Tech,
#    Hutcheson Hall, 403K, Blacksburg, VA 24061
#
# Source:
#   https://github.com/tbroderick/bnp_tutorial/tree/2016mlss
rm(list = ls())
# useful for sampling from a Dirichlet distribution
library(MCMCpack)
# maximum number of data points to draw
maxN = 1000
# use these parameters by default
K_default = 1000
a_default = 10/K_default
# note: default run with these parameters
# appears at the end
ex4_gen_largeK_count <- function(K,a) {
# Illustrates how number of clusters changes
# with Dirichlet-distributed component probabilities
#
# Args:
#  K: Dirichlet parameter vector length
#  a: Dirichlet parmeter (will be repeated K times)
#
# Returns:
#  Nothing
# make the Dirichlet draw
rhomx = rdirichlet(1,rep(a,K))
# another useful form of rho
rho = as.vector(rhomx)
# records which clusters have been sampled so far
uniq_draws = c()
# cluster samples in order of appearance (ooa)
ooa_clust = c()
for(N in 1:maxN) {
# draw a cluster assignment from the components
draw = sample(1:K, size=1, replace=TRUE, prob=rho)
# update info about cluster draws
uniq_draws = unique(c(uniq_draws, draw))
ooa = which(draw == uniq_draws)
ooa_clust = c(ooa_clust, ooa)
# plot cluster assignments in order of appearance
plot(seq(1,N),
ooa_clust,
xlab="Sample index",
ylab="Cluster by order of appearance",
ylim=c(0,max(10,length(uniq_draws))),
xlim=c(0,max(10,N)),
pch=19,
main=bquote(rho~"~Dirichlet"  # ~"("~.(a)~",...,"~.(a)~")"
~", K="~.(K))
)
# Generate a new draw for each press of "enter"
# Press 'x' when finished
line <- readline()
if(line == "x") return("done")
}
}
# default run with default parameters
ex4_gen_largeK_count(K_default, a_default)
# Description:
#   This script shows SMC method using SIR algorithm in Stochastic volatility model
#
# Details:
#   x.1 ~ N(0, \frac{\sigma^2}{1-\sigma^2})
#   x.n = phi x.{n-1} + v.n, v.n ~ N(0,\sigma^2), |sigma^2|<1
#   y.n = exp{gamma + x.n}w.n, w.n~ N(0,1)
#
# License: GNU v3
#
# Date: October 19, 2016
#
# Authors:
#    Rcode was written by Jiali Lin.
#    Depts. of Statistics, Virginia Tech,
#    Hutcheson Hall, 403K, Blacksburg, VA 24061
#
# References:
#    <http://www.apps.stat.vt.edu/zhu/teaching/2016/6474/6474_2016.htm>
rm(list = ls())
set.seed(100)
# Initial Value
Time = 100
sigma = 1
gamma = 1
phi = 0.9
x = rep(NA, Time)  # latent sequence
y = rep(NA, Time)  # observed sequence
x[1] = rnorm(1, mean = 0, sd = sigma/sqrt(1-phi^2))
for (i in 2:Time){
x[i] = phi*x[i-1] + rnorm(1, mean = 0, sd = sigma)
}
y = exp(gamma+x)*rnorm(Time, mean = 0, sd = 1)
# Plot the latent sequence and the observed sequence
# win.graph()
plot(1:Time, x, type = 'b', pch = 3, col = 'green',xlab = "Time",ylab = NULL)
points(1:Time, y, pch = 16, col = 'blue')
lines(1:Time, y,  col = 'blue')
legend(-1, -3, c('latent','observed'), col = c('green','blue'), pch = c(3,16), lty = c(1,1))
#  SMC method using SIR algorithm.
# -------------
N = 1000
Xs = matrix(NA, N, Time)
logweight = matrix(NA, N, Time)
weight.normalized = matrix(NA, N, Time)
mu.Xs = rep(NA, Time)
var.Xs = rep(NA, Time)
# sample from p(x_n|y_{1:n})
for (t in 1:Time){
if (t == 1){
# Xs[,t] = rnorm(N, mean= 0, sd = sigma/sqrt(1+phi^2))
Xs[,t] = rnorm(N, mean = 0, sd = sigma/sqrt(1-phi^2)) #**** zhunote: 1-phi^2
}else{
# propagate particles according to prior
Xs[,t] = rnorm(N, mean = phi*Xs[,t-1], sd = sigma)
}
logweight[, t] = dnorm(y[t], mean = 0, sd = exp(gamma+Xs[,t]), log = T)
weight.normalized[,t] = exp(logweight[,t])/sum(exp(logweight[,t]))
# Resample
Xs.resample = sample(Xs[,t], size = N, replace = T, prob=weight.normalized[,t])
Xs[,t] = Xs.resample
mu.Xs[t] = mean(Xs.resample)
var.Xs[t] = var(Xs.resample)
}
# win.graph()
plot(1:Time, x, type = 'b', pch = 3, col = 'grey', main = 'SIR Solution', ylim = c(-10, 20),xlab = "Time")
points(1:Time, y, pch = 16, col = 'blue')
lines(1:Time, y,  col = 'blue')
lines(mu.Xs-2*sqrt(var.Xs), lty = 2, col = 'black')
lines(mu.Xs+2*sqrt(var.Xs), lty = 2, col = 'black')
lines(mu.Xs, col = "red",lwd = 2)
points(mu.Xs, col = "red",lwd = 2, pch = 16)
legend(0, -5, c('latent','observed','post.mean', 'post.CI'),
pch = c(3, 16, 16, NA), col = c('grey','blue','red','black'),lty = c(1,1,1,2))
# Description:
#   This script shows SMC method using SIR algorithm in linear Gaussian state-space model
#
# Details:
#   x.1 ~ N(0, 1)
#   x.n = phi x.{n-1} + v.n, v.n ~ N(0,s.v^2)
#   y.n = x.n + w.n, w.n~ N(0,s.w^2)
#
# License: GNU v3
#
# Date: October 19, 2016
#
# Authors:
#    Rcode was written by Jiali Lin.
#    Depts. of Statistics, Virginia Tech,
#    Hutcheson Hall, 403K, Blacksburg, VA 24061
#
# References:
#    <http://www.apps.stat.vt.edu/zhu/teaching/2016/6474/6474_2016.htm>
rm(list = ls())
set.seed(100)
# Initial value
T = 100
S.v = 1
S.w = 1
phi = 0.95
x = rep(NA, T)  # latent sequence
y = rep(NA, T)  # observed sequence
x[1] = rnorm(1, mean = 0, sd = 1)
for (i in 2:T){
x[i] = phi*x[i-1] + rnorm(1, mean = 0, sd = S.v)
}
y = x + rnorm(T, mean = 0, sd = S.w)
# Initialize storage
N = 1000
Xs = matrix(NA, N, T)
logweight = matrix(NA, N, T)
weight.normalized = matrix(NA, N, T)
mu.Xs = rep(NA, T)
var.Xs = rep(NA, T)
for (t in 1:T){
if (t == 1){
Xs[,t] = rnorm(N, mean =  0, sd = 1)
}else{
# propagate particles according to prior
Xs[,t] = rnorm(N, mean = phi*Xs[,t-1], sd = S.v)
}
logweight[, t] = dnorm(y[t], mean = Xs[,t], sd = S.w, log = T)
weight.normalized[,t] = exp(logweight[,t])/sum(exp(logweight[,t]))
# Resample
Xs.resample = sample(Xs[,t], size = N, replace = T, prob = weight.normalized[,t])
Xs[,t] = Xs.resample
mu.Xs[t] = mean(Xs.resample)
var.Xs[t] = var(Xs.resample)
}
# win.graph()
plot(1:T, x, type = 'b', pch = 3, col = 'grey', main = 'SIR Solution', ylim = c(-4, 5))
points(1:T, y, pch = 16, col = 'blue')
lines(1:T, y,  col = 'blue')
lines(mu.Xs-2*sqrt(var.Xs), lty = 2, col = 'black')
lines(mu.Xs+2*sqrt(var.Xs), lty = 2, col = 'black')
lines(mu.Xs, col = "red",lwd = 2)
points(mu.Xs, col = "red",lwd = 2, pch = 16)
legend(10, -2.4, c('latent','observed','post.mean', 'post.CI'),
pch = c(3, 16, 16, NA), col = c('grey','blue','red','black'),lty = c(1,1,1,2))
# Description:
#   This script shows SMC method using SIR algorithm in linear Gaussian state-space model
#
# Details:
#   x.1 ~ N(0, 1)
#   x.n = phi x.{n-1} + v.n, v.n ~ N(0,s.v^2)
#   y.n = x.n + w.n, w.n~ N(0,s.w^2)
#
# License: GNU v3
#
# Date: October 19, 2016
#
# Authors:
#    Rcode was written by Jiali Lin.
#    Depts. of Statistics, Virginia Tech,
#    Hutcheson Hall, 403K, Blacksburg, VA 24061
#
# References:
#    <http://www.apps.stat.vt.edu/zhu/teaching/2016/6474/6474_2016.htm>
rm(list = ls())
set.seed(100)
# Initial value
T <- 100
S.v <- 1
S.w <- 1
phi <- 0.95
x <- rep(NA, T)  # latent sequence
y <- rep(NA, T)  # observed sequence
x[1] <- rnorm(1, mean = 0, sd = 1)
for (i in 2:T){
x[i] <- phi*x[i-1] + rnorm(1, mean = 0, sd = S.v)
}
y <- x + rnorm(T, mean = 0, sd = S.w)
# Initialize storage
N <- 1000
Xs <- matrix(NA, N, T)
logweight <- matrix(NA, N, T)
weight.normalized <- matrix(NA, N, T)
mu.Xs <- rep(NA, T)
var.Xs <- rep(NA, T)
for (t in 1:T){
if (t == 1){
Xs[, t] <- rnorm(N, mean =  0, sd = 1)
}else{
# propagate particles according to prior
Xs[, t] <- rnorm(N, mean = phi  *Xs[, t-1], sd = S.v)
}
logweight[, t] <- dnorm(y[t], mean = Xs[, t], sd = S.w, log = T)
weight.normalized[, t] <- exp(logweight[, t]) / sum(exp(logweight[, t]))
# Resample
Xs.resample <- sample(Xs[, t], size = N, replace = T, prob = weight.normalized[, t])
Xs[, t] <- Xs.resample
mu.Xs[t] <- mean(Xs.resample)
var.Xs[t] <- var(Xs.resample)
}
# win.graph()
plot(1:T, x, type = 'b', pch = 3, col = 'grey', main = 'SIR Solution', ylim = c(-4, 5))
points(1:T, y, pch = 16, col = 'blue')
lines(1:T, y,  col = 'blue')
lines(mu.Xs - 2 * sqrt(var.Xs), lty = 2, col = 'black')
lines(mu.Xs + 2 * sqrt(var.Xs), lty = 2, col = 'black')
lines(mu.Xs, col = "red", lwd = 2)
points(mu.Xs, col = "red", lwd = 2, pch = 16)
legend(10, -2.4, c('latent', 'observed', 'post.mean', 'post.CI'),
pch = c(3, 16, 16, NA), col = c('grey', 'blue', 'red', 'black'), lty = c(1, 1, 1, 2))
rm(list = ls())
# Description:
#   This script shows SMC method using SIR algorithm in linear Gaussian state-space model
#
# Details:
#   x.1 ~ N(0, 1)
#   x.n = phi x.{n-1} + v.n, v.n ~ N(0,s.v^2)
#   y.n = x.n + w.n, w.n~ N(0,s.w^2)
#
# License: GNU v3
#
# Date: October 19, 2016
#
# Authors:
#    Rcode was written by Jiali Lin.
#    Depts. of Statistics, Virginia Tech,
#    Hutcheson Hall, 403K, Blacksburg, VA 24061
#
# References:
#    <http://www.apps.stat.vt.edu/zhu/teaching/2016/6474/6474_2016.htm>
rm(list = ls())
set.seed(100)
# Initial value
T <- 100
S.v <- 1
S.w <- 1
phi <- 0.95
x <- rep(NA, T)  # latent sequence
y <- rep(NA, T)  # observed sequence
x[1] <- rnorm(1, mean = 0, sd = 1)
for (i in 2:T){
x[i] <- phi*x[i-1] + rnorm(1, mean = 0, sd = S.v)
}
y <- x + rnorm(T, mean = 0, sd = S.w)
# Initialize storage
N <- 1000
Xs <- matrix(NA, N, T)
logweight <- matrix(NA, N, T)
weight.normalized <- matrix(NA, N, T)
mu.Xs <- rep(NA, T)
var.Xs <- rep(NA, T)
for (t in 1:T){
if (t == 1){
Xs[, t] <- rnorm(N, mean =  0, sd = 1)
}else{
# propagate particles according to prior
Xs[, t] <- rnorm(N, mean = phi  *Xs[, t-1], sd = S.v)
}
logweight[, t] <- dnorm(y[t], mean = Xs[, t], sd = S.w, log = T)
weight.normalized[, t] <- exp(logweight[, t]) / sum(exp(logweight[, t]))
# Resample
Xs.resample <- sample(Xs[, t], size = N, replace = T, prob = weight.normalized[, t])
Xs[, t] <- Xs.resample
mu.Xs[t] <- mean(Xs.resample)
var.Xs[t] <- var(Xs.resample)
}
# win.graph()
plot(1:T, x, type = 'b', pch = 3, col = 'grey', main = 'SIR Solution', ylim = c(-4, 5))
points(1:T, y, pch = 16, col = 'blue')
lines(1:T, y,  col = 'blue')
lines(mu.Xs - 2 * sqrt(var.Xs), lty = 2, col = 'black')
lines(mu.Xs + 2 * sqrt(var.Xs), lty = 2, col = 'black')
lines(mu.Xs, col = "red", lwd = 2)
points(mu.Xs, col = "red", lwd = 2, pch = 16)
legend(10, -2.4, c('latent', 'observed', 'post.mean', 'post.CI'),
pch = c(3, 16, 16, NA), col = c('grey', 'blue', 'red', 'black'), lty = c(1, 1, 1, 2))
