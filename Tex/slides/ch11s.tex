\documentclass[10pt,mathserif]{beamer}

\usepackage{graphicx,amsmath,amssymb,tikz,psfrag,subfigure,bm}

\input defs.tex

%% formatting

\mode<presentation>
{
\usetheme{default}
}
\setbeamertemplate{navigation symbols}{}
\usecolortheme[rgb={0,0,0}]{structure}
\setbeamertemplate{itemize subitem}{--}
\setbeamertemplate{frametitle} {
	\begin{center}
	  {\large\bf \insertframetitle}
	\end{center}
}

\AtBeginSection[] 
{ 
	\begin{frame}<beamer> 
		\frametitle{Outline} 
		\tableofcontents[currentsection,currentsubsection] 
	\end{frame} 
} 

%% begin presentation

\title{\large \bfseries Mixture models and the EM algorithm}

\author{Jiali Lin\\[3ex]
Virginia Tech}

\date{\today}

\begin{document}

\frame{
\thispagestyle{empty}
\titlepage
}

\section{Latent variable models}
\begin{frame}{Latent Variable Models (LVMs)}
\begin{itemize}
    \item \textbf{Graphial model:} model dependence between two variables by adding an edge between them.
    \item \textbf{Latent variable:} assume that the observed variables are correlated because they arise from a hidden common ``cause".
    \item \textbf{Pros 1}: LVMs have fewer parameters than models that directly represent correlation in the visible space.
    \item \textbf{Pros 2}: good for compression of $\bm{x}$.
    \item \textbf{Cons}: harder to fit than models with no latent variables.
    %item \textbf{Probabilistic learning:} Estimate parameters of joint distribution $p(z,x)$ which maximize marginal probability $p(z)$.
\end{itemize}
\end{frame}

\section{K-means Clustering}
\begin{frame}{K-Means Objective: Compression}
\begin{itemize}
    \item Observed feature vectors: $\bm{x}_i\in\mathbb{R}^d, \quad i=1,\ldots,N$.
    \item Hidden cluster labels: $z_i\in\{1,2,\ldots,K\}, \quad i=1,\ldots,N$.
    \item Hidden cluster centers: $\bm{\mu}_k\in\mathbb{R}^d, \quad k=1,\ldots,K$.
    \begin{equation*}
        \begin{split}
            J(\bm{z},\bm{\mu}|\bm{x},K) & = \sum_{k=1}^K \sum_{i|z_i=k}\|\bm{x}_i-\bm{\mu}_k\|^2 = \sum_{i=1}^N \|\bm{x}_i-\bm{\mu}_{z_i}\|^2\\
            J(\bm{z},\bm{\mu}|\bm{x},K) & = \sum_{k=1}^K \sum_{i=1}^N z_{ik}\|\bm{x}_i-\bm{\mu}_k\|^2, \quad z_{ik} = \mathbb{I}(z_i = k)
        \end{split}
    \end{equation*}
    \item K-Means alternates between:
    \begin{itemize}
        \item $\bm{z}^{t} = \argmin_{\bm{z}} J(\bm{z},\bm{\mu}^{t-1}|\bm{x},K)$
        \item $\bm{\mu}^{t} = \argmin_{\bm{\mu}} J(\bm{z}^t,\bm{\mu}|\bm{x},K)$    
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{K-Means Algorithm}
Objective function: $J(\bm{z},\bm{\mu}|\bm{x},K) = \sum_{k=1}^K \sum_{i=1}^N z_{ik}\|\bm{x}_i-\bm{\mu}_k\|^2$.
\noindent\rule[-5pt]{\textwidth}{0.4pt}
{\footnotesize
\begin{tabbing}
    {\bf given} Choose random cluster centers. $\bm{\mu}^{(0)}$. \\*[\smallskipamount]
    {\bf repeat} \\
    \qquad \= 1.\ Assignment Step: $\bm{z}^{t} = \argmin_{\bm{z}} J(\bm{z},\bm{\mu}^{t-1}|\bm{x},K), \quad z^{(t)}_i = \argmin_k\|\bm{x}_i-\bm{\mu}^{(t-1)}_k\|^2$. \\
    \> 2.\ Mean Update Step: $\bm{\mu}^{t} = \argmin_{\bm{\mu}} J(\bm{z}^t,\bm{\mu}|\bm{x},K), \quad {\bm{\mu}^{(t)}_k} = \frac{1}{N^{(t)}_k}\sum_{i=1}^Nz_{ik}\bm{x}_i$. \\*[\smallskipamount]
    {\bf return} $z_{ik}$.
\end{tabbing}}
\noindent\rule[10pt]{\textwidth}{0.4pt}

\textbf{Step 1:} assign data to closest cluster centers, breaking ties arbitrarily. \\
\textbf{Step 2:} means of data assigned to each cluster center (least squares).  
\end{frame}

\begin{frame}{Illustration of K-Means}
\begin{figure}[h]
\centering     %%% not \center
\subfigure[]{\includegraphics[width=15mm]{{Figure9.1a}.pdf}}
\subfigure[]{\includegraphics[width=15mm]{{Figure9.1b}.pdf}}
\subfigure[]{\includegraphics[width=15mm]{{Figure9.1c}.pdf}}\\
\subfigure[]{\includegraphics[width=15mm]{{Figure9.1d}.pdf}}
\subfigure[]{\includegraphics[width=15mm]{{Figure9.1e}.pdf}}
\subfigure[]{\includegraphics[width=15mm]{{Figure9.1f}.pdf}}\\
\subfigure[]{\includegraphics[width=15mm]{{Figure9.1g}.pdf}}
\subfigure[]{\includegraphics[width=15mm]{{Figure9.1h}.pdf}}
\subfigure[]{\includegraphics[width=15mm]{{Figure9.1i}.pdf}}
\caption{Illustration of the K-means algorithm using the re-scaled Old Faithful data set. Figure generated by \texttt{KmeansDemoFaithful}.}
\end{figure}
\end{frame}

\begin{frame}{K-Means Implementation \& Properties}
\textbf{Initialization:} Choose random cluster centers $\bm{\mu}_{(0)}$
\begin{itemize}
    \item Should be distinct (breaking symmetry) and in ``region" of data.
    \item Common heuristic: randomly pick $K$ data points.
    \item K-Means++: randomly pick $K$ widely separated data points.
\end{itemize}
\textbf{Theoretical Guarantees:}
\begin{itemize}
    \item \textcolor{green}{Converges after finitely many iterations} $z^{(t+1)} = z^{(t)}$.
    \item \textcolor{red}{Worst-case convergence time poor (super-polynomial in $N$)}.
    \item \textcolor{red}{Different initializations may produce very different solutions}.
    \item \textcolor{red}{Converged objective may be arbitrarily worse than optimum}, \textcolor{green}{but smart initializations (K-Means++) do allow some guarantees}.
    \item In practice, can usually still find ``useful" local optima.
    \item Optimal reconstruction error always decreases with $K, 0$ if $K=N$.
\end{itemize}
\end{frame}

\section{Mixture models}
\begin{frame}{Gaussian Mixture Models}
\begin{itemize}
    \item Observed feature vectors: $\bm{x}_i\in\mathbb{R}^d, \quad i=1,\ldots,N$.
    \item Hidden cluster labels: $z_i\in\{1,2,\ldots,K\}, \quad i=1,\ldots,N$.
    \item Hidden cluster centers: $\bm{\mu}_k\in\mathbb{R}^d, \quad k=1,\ldots,K$. 
    \item Hidden mixture covariances: $\bm{\Sigma}_k\in\mathbb{R}^{d \times d}, \quad k=1,\ldots,K$.
    \item Hidden mixture probabilities: $\pi_k, \quad \sum_{i=1}^K \pi_k = 1$
    \item Gaussian mixture generative model: 
    \begin{equation*}
        \begin{split}
            p(z_i) & = \text{Cat}(z_i|\bm{\pi}) \\
            p(\bm{x}_i | z_i) & = N(\bm{x}_i | \bm{\mu}_{z_i}, \bm{\Sigma}_{z_i} )\\
            p(\bm{x}_i|\bm{\pi},\bm{\mu},\Sigma) & = \sum_{z_i=1}^K \pi_{z_i} N(\bm{x}_i | \bm{\mu}_{z_i}, \bm{\Sigma}_{z_i})
        \end{split}
    \end{equation*}
\end{itemize}
\end{frame}

\begin{frame}{Unsupervised Learning}
\begin{itemize}
    \item Learning: 
    \begin{equation*}
        \argmax_{\bm{\pi},\bm{\theta}} \ln p(\bm{\pi}) + \ln p(\bm{\theta}) + \sum_{i=1}^N \left[\sum_{z_i}p(z_i|\bm{\pi})p(\bm{x}_i|z_i,\bm{\theta})\right]
    \end{equation*}
    \item No notion of training and test data: labels are never observed.
    \item As before, maximize posterior probability of model parameters.
    \item For hidden variables associated with each observation, we marginalize over possible values rather than estimating.
    \begin{itemize}
        \item Fully accounts for uncertainty in these variables.
        \item There is one hidden variable per observation, so cannot perfectly estimate even with infinite data.
    \end{itemize}
    \item Must use generative model (discriminative degenerates).
    \item Learning is harder
    \begin{itemize}
        \item In fully observed iid settings, the log likelihood decomposes into a sum of local terms.
        \begin{equation*}
            \ell(\bm{\theta}) = \ln p(\bm{x},\bm{z}|\bm{\theta}) = \ln p(\bm{z}|\bm{\theta}_{\bm{z}}) + \ln p(\bm{x}|\bm{z},\bm{\theta}_{\bm{x}})
        \end{equation*}
        \item With latent variables, all the parameters become coupled together via marginalization
        \begin{equation*}
            \ell(\bm{\theta}) = \ln \sum p(\bm{x} , \bm{z}|\bm{\theta}) = \ln \sum p(\bm{z}|\bm{\theta}_{\bm{z}}) p(\bm{x}|\bm{z}, \bm{\theta}_{\bm{x}} )
        \end{equation*}
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Singularities: ML for Gaussian Mixtures}
\begin{figure}[h]
\centering     %%% not \center
\subfigure[]{\includegraphics[width=40mm]{{Figure9.6}.pdf}}
\subfigure[]{\includegraphics[width=40mm]{{Figure9.7}.pdf}}
\caption{(a) Graphical representation of a Gaussian mixture model. (b) Illustration of how singularities in the likelihood function arise with mixtures
of Gaussians.}
\end{figure}
\end{frame}

\begin{frame}{Unsupervised Learning Algorithms }
\begin{itemize}
\item Initialization: Randomly select starting parameters.
\item Estimation: Given parameters, infer likely hidden data.
    \begin{itemize}
        \item Similar to testing phase of supervised learning.
    \end{itemize}
\item Learning: Given hidden \& observed data, find likely parameters.
    \begin{itemize}
        \item Similar to training phase of supervised learning.
    \end{itemize}
\item Iteration: Alternate estimation \& learning until convergence.
\end{itemize}
\end{frame}

\section{The EM algorithm}
\begin{frame}{Expectation Maximization (EM)}
\textbf{Goal:} maximize the likelihood function $p(\bm{X}|\bm{\theta})$ with respect to $\bm{\theta}$. 
\begin{itemize}
    \item \textbf{Input:} $p(\bm{X}, \bm{Z}|\bm{\theta}), p(\bm{Z}|\bm{X},\bm{\theta})$.
    \item Choose an initial setting for the parameters $\bm{\theta}^{\text{old}}$.
    \item \textbf{E step.} Evaluate $p(\bm{Z}|\bm{X}, \bm{\theta}^{\text{old}})$.
    \item \textbf{M step.} Evaluate $\bm{\theta}^{\text{new}}$ given by 
        \begin{equation*}
            \bm{\theta}^{\text{new}} = \max_{\bm{\theta}}Q(\bm{\theta}, \bm{\theta}^{\text{old}})
        \end{equation*}
    where $Q(\bm{\theta}, \bm{\theta}^{\text{old}}) = \sum_{\bm{Z}} p(\bm{Z}|\bm{X},\bm{\theta}^{\text{old}}) \ln p(\bm{X},\bm{Z}|\bm{\theta})$
    \item Check for convergence of either the log likelihood or the parameter values. If the convergence criterion is not satisfied, then let
    \begin{equation*}
        \bm{\theta}^{\text{old}} \leftarrow  \bm{\theta}^{\text{new}}
    \end{equation*}
    and return to step 2.
\end{itemize}
\end{frame}

\begin{frame}{Example: EM for Gaussian Mixtures}
\begin{itemize}
    \item Initialize the means $\bm{\mu}_k$, covariances $\bm{\Sigma}_k$ and mixing coefficients $\pi_k$, and evaluate the initial value of the log likelihood.
    \item \textbf{E step.} Evaluate the responsibilities (the expected value of the sufficient statistics of the hidden variables) using the current parameter values
        \begin{equation*}
            \gamma(z_{ik}) = p(z_i = k | \bm{x}_i, \bm{\pi}, \bm{\theta}) = \frac{\pi_k N(\bm{x}_n|\bm{\mu}_k,\bm{\Sigma}_k)}{\sum_{j=1}^K \pi_j N(\bm{x}_n|\bm{\mu}_j,\bm{\Sigma}_j) }
        \end{equation*}
    \item \textbf{M step.} Re-estimate the parameters using the current responsibilities (i.e. expected value of the hidden variables)
        \begin{equation*}
            \begin{split}
                \bm{\mu}_k^{\text{new}} & = \frac{1}{N_k}\sum_{n=1}^N\gamma(z_{ik})\bm{x}_n\\
                \bm{\Sigma}_k^{\text{new}} & = \frac{1}{N_k}\sum_{n=1}^N\gamma(z_{ik})(\bm{x}_n - \bm{\mu}_k)(\bm{x}_n - \bm{\mu}_k)^T\\
                \pi_k^{\text{new}} & = \frac{N_k}{N}
            \end{split}
        \end{equation*}
    where $N_k = \sum_{n=1}^N\gamma(z_{ik})$.
\end{itemize}
\end{frame}

\begin{frame}{Example: EM for Gaussian Mixtures(cont'd)}
\begin{itemize}
    \item Evaluate the log likelihood
        \begin{equation*}
            \ln p(\bm{X}|\bm{\mu},\bm{\Sigma},\pi) = \sum_{n=1}^N\ln\{\sum_{k=1}^K \pi_k N(\bm{x}_n|\bm{\mu}_k,\bm{\Sigma}_k)\}
        \end{equation*}
    and check for convergence of  the parameters or the log likelihood. If the convergence criterion is not satisfied return to step 2.
\end{itemize}
\end{frame}


\begin{frame}{Illustration of EM Algorithm for GGM}
\begin{figure}[h]
\centering     %%% not \center
\subfigure[]{\includegraphics[width=20mm]{{Figure9.8a}.pdf}}
\subfigure[]{\includegraphics[width=20mm]{{Figure9.8b}.pdf}}
\subfigure[]{\includegraphics[width=20mm]{{Figure9.8c}.pdf}}\\
\subfigure[]{\includegraphics[width=20mm]{{Figure9.8d}.pdf}}
\subfigure[]{\includegraphics[width=20mm]{{Figure9.8e}.pdf}}
\subfigure[]{\includegraphics[width=20mm]{{Figure9.8f}.pdf}}
\caption{Illustration of the EM algorithm using the Old Faithful set. Figure generated by \texttt{MixGaussDemoFaithful}.}
\end{figure}
\end{frame}

\begin{frame}{EM as Lower Bound Maximization}
\begin{equation*}
    \begin{split}
        \ln p(\bm{x}|\bm{\theta}) & = \ln (\sum_{\bm{z}} p(\bm{x},\bm{z}|\bm{\theta})) = \ln (\sum_{\bm{z}} q(\bm{z}) \frac{p(\bm{x},\bm{z}|\bm{\theta}))}{q(\bm{z})}\\
        & \geq \sum_{\bm{z}} q(\bm{z}) \ln (\frac{p(\bm{x},\bm{z}|\bm{\theta})}{q(\bm{z})}) \quad\text{(Jensen's Inequality)}\\
        & \geq \sum_{\bm{z}} q(\bm{z}) \ln p(\bm{x},\bm{z}|\bm{\theta}) -\sum_{\bm{z}} q(\bm{z}) \ln q(\bm{z}) = L(q,\bm{\theta})
    \end{split}
\end{equation*}

\begin{itemize}
\item Initialization: Randomly select starting parameters $\bm{\theta}_{(0)}$.
\item Inference: Given parameters, infer likely hidden data.
    \begin{equation*}
        q^{(t)} = \argmax_q L(q,\bm{\theta}^{(t-1)})
    \end{equation*}
\item Learning: Given hidden \& observed data, find likely parameters.
    \begin{equation*}
        \bm{\theta}^{(t)} = \argmax_{\bm{\theta}} L(q^{(t)},\bm{\theta})
    \end{equation*}
\item Iteration: Alternate estimation \& learning until convergence.
\end{itemize}
\end{frame}

\begin{frame}{Lower Bounds on Marginal Likelihood}
\begin{figure}[h]
\centering
\includegraphics[width=0.75\textwidth]{{Figure9.11}.pdf}
%\caption{}
\end{figure}
\end{frame}

\begin{frame}{EM: Expectation Step}
\begin{equation*}
    \begin{split}
        \ln p(\bm{x}|\bm{\theta}) & \geq \sum_{z} q(z) \ln p(\bm{x},\bm{z}|\bm{\theta}) -\sum_{\bm{z}} q(\bm{z}) \ln q(\bm{z}) = L(q,\bm{\theta})\\
        q^{(t)} & = \argmax_q L(q,\bm{\theta}^{(t-1)})
    \end{split}
\end{equation*}
\begin{itemize}
    \item One can also show this result using variational calculus
    \begin{equation*}
        \ln p(\bm{x}|\bm{\theta}) - \ln q(\bm{z}) = L(q,\bm{\theta}) = \text{KL}(q||p(\bm{z}|\bm{x},\bm{\theta}))
    \end{equation*}
    \item General solution, for any probabilistic model
    \begin{equation*}
        q^{(t)} = \argmax_q L(q,\bm{\theta}^{(t-1)})
    \end{equation*}
    \item For mixture models, data independent given parameters
    \begin{equation*}
        \begin{split}
            p(z_i|\bm{\pi}) & = \text{Cat}(z_i|\bm{\pi}) \\
            p(\bm{x}_i | z_i, \bm{\theta}) & = p(\bm{x}_i|\bm{\theta}_{z_i})\\
            \gamma(z_{ik}) & = p(z_i = k | \bm{x}_i, \bm{\pi}, \bm{\theta}) = \frac{\pi_k N(\bm{x}_n|\bm{\mu}_k,\bm{\Sigma}_k)}{\sum_{j=1}^K \pi_j N(\bm{x}_n|\bm{\mu}_j,\bm{\Sigma}_j) }
        \end{split}
    \end{equation*}
\end{itemize}
\end{frame}

\begin{frame}{Illustration of the E step}
\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{{Figure9.12}.pdf}
\caption{Illustration of the E step of the EM algorithm. The $q$ distribution is set equal to the posterior distribution for the current parameter values $\bm{\theta}^{\text{old}}$, causing the lower bound to move up to the same value as the log likelihood function, with the KL divergence vanishing.
}
\end{figure}
\end{frame}

\begin{frame}{EM: Maximization Step}
\begin{equation*}
    \begin{split}
        \ln p(\bm{x}|\bm{\theta}) & \geq \sum_{\bm{z}} q(\bm{z}) \ln p(\bm{x},\bm{z}|\bm{\theta}) -\sum_{\bm{z}} q(\bm{z}) \ln q(\bm{z}) = L(q,\bm{\theta})\\
        \bm{\theta}^{(t)} & = \argmax_{\bm{\theta}} L(q^{(t)},\bm{\theta}) = \argmax_{\bm{\theta}} = \sum_{\bm{z}} q(\bm{z})\ln p(\bm{x},\bm{z}|\bm{\theta})
    \end{split}
\end{equation*}
\begin{itemize}
    \item Unlike E-step, no simplified general solution.
    \item Applying to GMM
    \begin{equation*}
    \begin{split}
        \bm{\mu}_k^{\text{new}} & = \frac{1}{N_k}\sum_{n=1}^N\gamma(z_{ik})\bm{x}_n\\
        \bm{\Sigma}_k^{\text{new}} & = \frac{1}{N_k}\sum_{n=1}^N\gamma(z_{ik})(\bm{x}_n - \bm{\mu}_k)(\bm{x}_n - \bm{\mu}_k)^T\\
        \pi_k^{\text{new}} & = \frac{N_k}{N}
    \end{split}
\end{equation*}
\end{itemize}
\end{frame}

\begin{frame}{Illustration of the M step}
\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{{Figure9.13}.pdf}
\caption{Illustration of the M step of the EM algorithm. The distribution $q(\bm{Z})$ is held fixed and the lower bound $L(q,\bm{\theta})$ is maximized with respect to the parameter vector $\bm{\theta}$ to give a revised value $\bm{\theta}^{\text{new}}$. Because the KL divergence is nonnegative, this causes the log likelihood $\ln p(\bm{X}|\bm{\theta})$ to increase by at least as much as the lower bound does.}
\end{figure}
\end{frame}

\begin{frame}{EM: A Sequence of Lower Bounds}
\begin{figure}[h]
\centering
\includegraphics[width=0.75\textwidth]{{Figure9.14}.pdf}
\caption{The EM algorithm involves alternately computing a lower bound on the log likelihood for the current parameter values and then maximizing this bound to obtain the new parameter values.}
\end{figure}
\end{frame}

\end{document}
